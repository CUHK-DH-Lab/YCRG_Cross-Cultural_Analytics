{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CUHK-DH-Lab/CUHK-DH-Lab.github.io/blob/main/More_texts_for_CRF_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 1: The Local Processing Loop**\n",
        "\n",
        "This first block of code acts as your \"Digital Scrubbing Station.\" Since OCR from early modern texts is often \"noisy\"—full of long 's' (ſ), weird ligatures, and broken lines—this section uses your local CPU to handle the heavy lifting before we involve any AI.\n",
        "The Directory Loop: The main() function uses Path.glob('*.txt') to grab every file in your Other_texts folder. It iterates through them one by one, ensuring that no matter how many documents you upload, they all get processed automatically.\n",
        "The Pipeline: For every file, it runs a process() function. This is a sequence of regex (Regular Expression) \"surgeries\" that:\n",
        "Normalize Glyphs: Swaps historical characters (like æ) for modern equivalents.\n",
        "De-hyphenate: Intelligently joins words split across lines—crucial for making the text readable for the LLM later.\n",
        "Structural Cleanup: Strips out \"page furniture\" like headers and page numbers that would otherwise confuse a translation or analysis model."
      ],
      "metadata": {
        "id": "GR3Wc3s5HLKX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pbgt9gxG1lp",
        "outputId": "9f215b6d-a8b8-4bdf-8698-1c124769d116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting cleanup on 1 files...\n",
            "Successfully processed: bub_gb_vrNA0Xt0g3UC_djvu.txt\n",
            "\n",
            "Finished! Cleaned files are in: /content/Cleaned_texts\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "from pathlib import Path\n",
        "import os\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# --- Configuration ---\n",
        "# This looks for a folder named 'Other_texts' in your Colab file pane\n",
        "INPUT_DIR = Path('/content/Sinensis_Texts1')\n",
        "OUTPUT_DIR = Path('/content/Cleaned_texts')\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Map curly quotes/dashes/ligatures/OCR errors\n",
        "PUNCT_MAP = {\n",
        "    '\\u017F': 's',         # long s (ſ)\n",
        "    'æ': 'ae', 'Æ': 'AE',\n",
        "    'œ': 'oe', 'Œ': 'OE',\n",
        "    '\\u2018': \"'\", '\\u2019': \"'\", '\\u201A': ',', '\\u201B': \"'\",\n",
        "    '\\u201C': '\"', '\\u201D': '\"', '\\u201E': '\"',\n",
        "    '\\u2013': '-', '\\u2014': '-', '\\u2212': '-',   # en/em/minus → hyphen\n",
        "    '\\u00AD': '-',                                 # soft hyphen → hyphen\n",
        "    '\\u00A0': ' ', '\\u2002': ' ', '\\u2003': ' ', '\\u2009': ' ', # spaces\n",
        "}\n",
        "\n",
        "# --- Patterns for Removal ---\n",
        "ROMAN_LINE = re.compile(r'^\\s*[IVXLCDM]+\\.*\\s*$')\n",
        "ARABIC_PAGE = re.compile(r'^\\s*\\d{1,4}\\s*$')\n",
        "FILLER_LINE = re.compile(r'^\\s*[•·.\\u2022]{3,}\\s*$')\n",
        "SPACED_SMALLCAPS = re.compile(r'\\b(?:[A-Z]\\s){2,}[A-Z]\\b')\n",
        "\n",
        "# --- OCR Fix Utilities ---\n",
        "\n",
        "def normalize_glyphs(s: str) -> str:\n",
        "    \"\"\"Standardize unicode punctuation, ligatures, and OCR noise characters.\"\"\"\n",
        "    for k, v in PUNCT_MAP.items():\n",
        "        s = s.replace(k, v)\n",
        "    s = unicodedata.normalize('NFKC', s)\n",
        "    return s\n",
        "\n",
        "def strip_scanner_artifacts(s: str) -> str:\n",
        "    \"\"\"Remove common stray OCR artifacts like escaped parentheses/backslashes.\"\"\"\n",
        "    s = s.replace('\\\\(', '(').replace('\\\\)', ')')\n",
        "    s = re.sub(r'\\\\{2,}', r'\\\\', s)\n",
        "    return s\n",
        "\n",
        "def fix_spaced_smallcaps(s: str) -> str:\n",
        "    \"\"\"Join sequences like \"L U D O V I C O\" -> \"LUDOVICO\".\"\"\"\n",
        "    def _join(m):\n",
        "        return m.group(0).replace(' ', '')\n",
        "    return SPACED_SMALLCAPS.sub(_join, s)\n",
        "\n",
        "def remove_page_numbers_and_fillers(text: str) -> str:\n",
        "    \"\"\"Removes lines matching page number formats or filler dots.\"\"\"\n",
        "    lines = text.splitlines()\n",
        "    kept = []\n",
        "    for L in lines:\n",
        "        S = L.strip()\n",
        "        if not S:\n",
        "            kept.append(L)\n",
        "            continue\n",
        "        if ARABIC_PAGE.match(S) or ROMAN_LINE.match(S) or FILLER_LINE.match(S):\n",
        "            continue\n",
        "        kept.append(L)\n",
        "    return \"\\n\".join(kept)\n",
        "\n",
        "def smart_dehyphenate(s: str) -> str:\n",
        "    \"\"\"Join end-of-line hyphens when the next line starts with lowercase.\"\"\"\n",
        "    s = re.sub(r'([A-Za-zÀ-ÿ]{2,})-\\n([a-zà-ÿ]{2,})', r'\\1\\2', s)\n",
        "    s = re.sub(r'([A-Za-zÀ-ÿ]{2,})-\\n([^\\S\\r\\n]*[a-zà-ÿ]{1,})', r'\\1\\2', s)\n",
        "    s = re.sub(r'(\\w)-(\\w)', lambda m: m.group(1)+m.group(2) if len(m.group(1))>1 and len(m.group(2))>1 else m.group(0), s)\n",
        "    return s\n",
        "\n",
        "def normalize_whitespace_preserve_structure(s: str) -> str:\n",
        "    \"\"\"Collapses extra spaces but preserves paragraph breaks.\"\"\"\n",
        "    paragraphs = re.split(r'\\n{2,}', s)\n",
        "    cleaned = []\n",
        "    for p in paragraphs:\n",
        "        p1 = re.sub(r'[ \\t]+', ' ', p.strip())\n",
        "        p1 = re.sub(r'\\n{3,}', '\\n\\n', p1)\n",
        "        cleaned.append(p1)\n",
        "    out = \"\\n\\n\".join([x for x in cleaned if x])\n",
        "    out = re.sub(r'[ \\t]+$', '', out, flags=re.MULTILINE)\n",
        "    return out\n",
        "\n",
        "def process(text: str) -> str:\n",
        "    \"\"\"Main processing pipeline.\"\"\"\n",
        "    t = normalize_glyphs(text)\n",
        "    t = strip_scanner_artifacts(t)\n",
        "    t = remove_page_numbers_and_fillers(t)\n",
        "    t = smart_dehyphenate(t)\n",
        "    t = fix_spaced_smallcaps(t)\n",
        "    t = normalize_whitespace_preserve_structure(t)\n",
        "    return t\n",
        "\n",
        "# --- Main Execution Loop ---\n",
        "\n",
        "def main():\n",
        "    if not INPUT_DIR.exists():\n",
        "        print(f\"!!! Error: Folder '{INPUT_DIR}' not found. Please create it and upload your files.\")\n",
        "        return\n",
        "\n",
        "    # Find all .txt files\n",
        "    files = list(INPUT_DIR.glob('*.txt'))\n",
        "\n",
        "    if not files:\n",
        "        print(f\"No .txt files found in {INPUT_DIR}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Starting cleanup on {len(files)} files...\")\n",
        "\n",
        "    for file_path in files:\n",
        "        try:\n",
        "            # Read input\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                raw_content = f.read()\n",
        "\n",
        "            # Clean content\n",
        "            cleaned_content = process(raw_content)\n",
        "\n",
        "            # Save to new folder with suffix\n",
        "            output_name = f\"{file_path.stem}_Cleaned.txt\"\n",
        "            output_path = OUTPUT_DIR / output_name\n",
        "\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(cleaned_content)\n",
        "\n",
        "            print(f\"Successfully processed: {file_path.name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path.name}: {e}\")\n",
        "\n",
        "    print(f\"\\nFinished! Cleaned files are in: {OUTPUT_DIR}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Section 2: The Async AI Correction Engine**\n",
        "\n",
        "After text cleanup, the code sends the text to Gemini 2.5 Flash for linguistic correction. To improve the speed of processing large documents, the code uses Asynchronous (Async) processing.\n",
        "smart_chunk_text: This function divides the text into manageable chunks. The function splits the text into pieces, approximately 10,000 characters long, while preserving paragraph breaks.\n",
        "The Power of asyncio:\n",
        "Concurrency: Instead of sending chunks sequentially, async allows multiple requests to be sent concurrently, as defined by MAX_CONCURRENCY.\n",
        "nest_asyncio: nest_asyncio.apply() enables the Gemini API to run inside the Colab/Jupyter notebook.\n",
        "Robustness (Retries & Backoff): The call_with_retries function handles potential API call failures. If the server indicates rate limits, the code pauses, waits, and retries automatically."
      ],
      "metadata": {
        "id": "xNNQbtDZHVqG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvsJmuqSJgXo",
        "outputId": "2e04f5af-3645-4d85-f892-86f77746e7af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.12/dist-packages (1.71.1)\n",
            "Requirement already satisfied: vertexai in /usr/local/lib/python3.12/dist-packages (1.71.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.29.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.47.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.27.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (5.29.6)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (26.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (3.40.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.1.2)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.12.3)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (0.17.0)\n",
            "\u001b[33mWARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.78.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.71.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.8.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (2.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-aiplatform vertexai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfWSO8ZbPd8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57588cf6-59ea-49aa-cf77-e42a2d87a8b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "CYZo6db6G6hM",
        "outputId": "9b69a9b2-0f03-40b3-c72c-d52230e866f9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unsupported region for Vertex AI, select from frozenset({'europe-west9', 'europe-central2', 'northamerica-northeast1', 'us-east5', 'europe-west4', 'us-west2', 'asia-northeast3', 'southamerica-west1', 'us-west1', 'us-west4', 'europe-west12', 'asia-southeast1', 'asia-east1', 'northamerica-northeast2', 'australia-southeast2', 'asia-east2', 'europe-west1', 'southamerica-east1', 'asia-northeast2', 'asia-south1', 'asia-southeast2', 'us-east4', 'asia-northeast1', 'us-south1', 'europe-west2', 'me-west1', 'europe-north1', 'us-central1', 'europe-southwest1', 'us-east1', 'me-central2', 'me-central1', 'europe-west6', 'europe-west8', 'europe-west3', 'us-west3', 'australia-southeast1', 'africa-south1'})",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2138994325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2138994325.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_vertex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0msemaphore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSemaphore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_CONCURRENCY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2138994325.py\u001b[0m in \u001b[0;36minit_vertex\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Credentials file not found: {JSON_FILE_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GOOGLE_APPLICATION_CREDENTIALS'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSON_FILE_PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPROJECT_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCATION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGEMINI_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Gemini Model '{GEMINI_MODEL}' initialized successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/initializer.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, project, location, experiment, experiment_description, experiment_tensorboard, staging_bucket, credentials, encryption_spec_key_name, network, service_account, api_endpoint, api_key, api_transport, request_metadata)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{api_transport} is not supported with API keys. \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexperiment_description\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/utils/__init__.py\u001b[0m in \u001b[0;36mvalidate_region\u001b[0;34m(region)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mregion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregion\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUPPORTED_REGIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;34mf\"Unsupported region for Vertex AI, select from {constants.SUPPORTED_REGIONS}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Unsupported region for Vertex AI, select from frozenset({'europe-west9', 'europe-central2', 'northamerica-northeast1', 'us-east5', 'europe-west4', 'us-west2', 'asia-northeast3', 'southamerica-west1', 'us-west1', 'us-west4', 'europe-west12', 'asia-southeast1', 'asia-east1', 'northamerica-northeast2', 'australia-southeast2', 'asia-east2', 'europe-west1', 'southamerica-east1', 'asia-northeast2', 'asia-south1', 'asia-southeast2', 'us-east4', 'asia-northeast1', 'us-south1', 'europe-west2', 'me-west1', 'europe-north1', 'us-central1', 'europe-southwest1', 'us-east1', 'me-central2', 'me-central1', 'europe-west6', 'europe-west8', 'europe-west3', 'us-west3', 'australia-southeast1', 'africa-south1'})"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "import nest_asyncio\n",
        "\n",
        "# Enable async support for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Configuration ---\n",
        "PROJECT_ID = 'renaissance-ocr'\n",
        "LOCATION = 'us-central1'\n",
        "GEMINI_MODEL = 'gemini-2.5-flash'\n",
        "JSON_FILE_PATH = 'renaissance-ocr-4aabe5b8dc65.json'\n",
        "\n",
        "INPUT_DIR = Path('/content/Cleaned_texts')\n",
        "OUTPUT_DIR = Path('/content/Gemini_Corrected_texts')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "TARGET_CHUNK_SIZE = 5000  # Character count per API call\n",
        "MAX_CONCURRENCY = 5        # Number of simultaneous chunks\n",
        "MAX_RETRIES = 5\n",
        "INITIAL_BACKOFF = 2.0      # Seconds for rate-limit retry\n",
        "\n",
        "from vertexai.generative_models import SafetySetting, HarmCategory, HarmBlockThreshold\n",
        "\n",
        "# Define loose safety settings\n",
        "SAFETY_SETTINGS = [\n",
        "    SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=HarmBlockThreshold.BLOCK_NONE),\n",
        "    SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),\n",
        "    SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=HarmBlockThreshold.BLOCK_NONE),\n",
        "    SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=HarmBlockThreshold.BLOCK_NONE),\n",
        "]\n",
        "\n",
        "# --- Prompts ---\n",
        "SYSTEM_STEER = \"You are a specialized OCR correction engine for early modern Latin. Output ONLY the corrected text. Do not provide lists of changes, explanations, or commentary.\"\n",
        "\n",
        "CORRECTION_PROMPT = \"\"\"\n",
        "TASK: Correct ONLY OCR errors in the input text. Do not rewrite or alter authentic early modern usage.\n",
        "\n",
        "RULES:\n",
        "1. Preserve original wording, grammar, and syntax. Fix ONLY misreads, broken words, or OCR punctuation noise.\n",
        "2. Minimal normalization: u=vowel, v=consonant; use 'i' only (no 'j'); ae/oe ligatures; replace & with 'et'.\n",
        "3. DE-HYPHENATE: Join all words broken across line breaks. Remove the hyphen and merge the fragments.\n",
        "4. Keep paragraph breaks and capitalization exactly as in the input.\n",
        "5. Remove page furniture (headers, catchwords, signature marks like 'A ij').\n",
        "6. Preserve exotic/Chinese romanizations (e.g., 'Tai Ki Gin') exactly as printed.\n",
        "7. CRITICAL: Output ONLY the corrected text. No explanations. No bullet points.\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Utils\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "def init_vertex() -> GenerativeModel:\n",
        "    if not os.path.exists(JSON_FILE_PATH):\n",
        "        raise FileNotFoundError(f\"Credentials file not found: {JSON_FILE_PATH}\")\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = JSON_FILE_PATH\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "    model = GenerativeModel(GEMINI_MODEL)\n",
        "    print(f\"Gemini Model '{GEMINI_MODEL}' initialized successfully.\")\n",
        "    return model\n",
        "\n",
        "def smart_chunk_text(text: str, target_size: int) -> List[str]:\n",
        "    \"\"\"Splits text into chunks, attempting to break at double newlines (paragraphs).\"\"\"\n",
        "    paras = text.split('\\n\\n')\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for p in paras:\n",
        "        if current_length + len(p) > target_size and current_chunk:\n",
        "            chunks.append('\\n\\n'.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.append(p)\n",
        "        current_length += len(p)\n",
        "    if current_chunk:\n",
        "        chunks.append('\\n\\n'.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "async def call_with_retries(coro_factory):\n",
        "    attempt = 0\n",
        "    backoff = INITIAL_BACKOFF\n",
        "    last_exc = None\n",
        "    while attempt <= MAX_RETRIES:\n",
        "        try:\n",
        "            return await coro_factory()\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            if \"429\" in str(e) or \"500\" in str(e) or \"503\" in str(e):\n",
        "                attempt += 1\n",
        "                if attempt > MAX_RETRIES: break\n",
        "                await asyncio.sleep(backoff)\n",
        "                backoff *= 2\n",
        "            else:\n",
        "                raise e\n",
        "    raise last_exc\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Core Correction Logic\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "async def correct_ocr_chunk_async(model_client: GenerativeModel,\n",
        "                                  chunk_text: str,\n",
        "                                  chunk_id: int,\n",
        "                                  filename: str,\n",
        "                                  semaphore: asyncio.Semaphore) -> Tuple[int, str]:\n",
        "    \"\"\"Corrects a chunk and joins multi-part responses if they occur.\"\"\"\n",
        "    full_prompt = f\"{SYSTEM_STEER}\\n\\n{CORRECTION_PROMPT}\\n\\nTEXT TO CORRECT:\\n{chunk_text}\"\n",
        "\n",
        "    async def single_call():\n",
        "        async with semaphore:\n",
        "            return await model_client.generate_content_async(\n",
        "                full_prompt,\n",
        "                generation_config={\"temperature\": 0.1, \"max_output_tokens\": 8192}\n",
        "            )\n",
        "\n",
        "    try:\n",
        "        response = await call_with_retries(single_call)\n",
        "\n",
        "        # FIX: Join all parts together to prevent 'Multiple content parts' error\n",
        "        # Navigate through candidates -> content -> parts\n",
        "        if response.candidates and response.candidates[0].content.parts:\n",
        "            corrected = \"\".join(part.text for part in response.candidates[0].content.parts).strip()\n",
        "        else:\n",
        "            corrected = chunk_text\n",
        "\n",
        "        if not corrected:\n",
        "            corrected = chunk_text\n",
        "\n",
        "        print(f\"  [Chunk {chunk_id:03d}] ✓ {filename}\")\n",
        "        return chunk_id, corrected\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  [ERROR] {filename} Chunk {chunk_id}: {e} -> returning original\")\n",
        "        return chunk_id, chunk_text\n",
        "\n",
        "async def process_file(file_path: Path, model_client: GenerativeModel, semaphore: asyncio.Semaphore):\n",
        "    print(f\"\\nSTARTING FILE: {file_path.name}\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    chunks = smart_chunk_text(content, TARGET_CHUNK_SIZE)\n",
        "    tasks = [correct_ocr_chunk_async(model_client, c, i, file_path.name, semaphore) for i, c in enumerate(chunks)]\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    results.sort(key=lambda x: x[0])\n",
        "\n",
        "    final_text = \"\\n\\n\".join(text for _, text in results)\n",
        "\n",
        "    output_path = OUTPUT_DIR / f\"Gemini_{file_path.name}\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        f_out.write(final_text)\n",
        "    print(f\"DONE: {file_path.name}\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Main Execution\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "async def main():\n",
        "    model = init_vertex()\n",
        "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
        "\n",
        "    if not INPUT_DIR.exists():\n",
        "        print(f\"Input directory {INPUT_DIR} not found.\")\n",
        "        return\n",
        "\n",
        "    files = sorted(list(INPUT_DIR.glob('*.txt')))\n",
        "    if not files:\n",
        "        print(\"No .txt files found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(files)} files. Processing chunks in parallel...\")\n",
        "\n",
        "    # Process files one by one, chunks in parallel\n",
        "    for file_path in files:\n",
        "        await process_file(file_path, model, semaphore)\n",
        "\n",
        "    print(f\"\\n--- ALL TASKS COMPLETE. Check folder: {OUTPUT_DIR} ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        },
        "id": "pRbCF3KF68m7",
        "outputId": "072d111f-53fa-4014-95fe-a66202911e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.63.0)\n",
            "Collecting google-genai\n",
            "  Downloading google_genai-1.64.0-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (9.1.4)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.3.1)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from google-genai) (3.13.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->google-genai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->google-genai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->google-genai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->google-genai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->google-genai) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->google-genai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->google-genai) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.6.2)\n",
            "Downloading google_genai-1.64.0-py3-none-any.whl (728 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.8/728.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-genai\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.63.0\n",
            "    Uninstalling google-genai-1.63.0:\n",
            "      Successfully uninstalled google-genai-1.63.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.25.0 requires google-cloud-aiplatform[agent-engines]<2.0.0,>=1.132.0, but you have google-cloud-aiplatform 1.71.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-genai-1.64.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "6552f82c6e6647e08c1cf567c94d995e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import google.genai, sys; print(google.genai.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMobqAt4BcN4",
        "outputId": "bc68798f-9da7-4dce-da96-c535c1c011d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.64.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "OCR correction with Gemini 3 Flash (preview) using the Google Gen AI SDK (async).\n",
        "Backward-compatible with SDKs that do NOT support `system_instruction=` in generate_content().\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "import nest_asyncio\n",
        "\n",
        "# Unified Google Gen AI SDK\n",
        "from google import genai\n",
        "\n",
        "# Enable async support for Colab / notebooks\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Backend selection ---\n",
        "# Vertex AI (service account): USE_VERTEX=True, requires PROJECT_ID, LOCATION, and GOOGLE_APPLICATION_CREDENTIALS\n",
        "# Developer API (API key):     USE_VERTEX=False, requires GOOGLE_API_KEY or GEMINI_API_KEY\n",
        "USE_VERTEX = True\n",
        "\n",
        "# Vertex AI settings (used only when USE_VERTEX=True)\n",
        "PROJECT_ID = \"renaissance-ocr\"\n",
        "LOCATION = \"us-central1\"\n",
        "SERVICE_ACCOUNT_JSON = \"renaissance-ocr-4aabe5b8dc65.json\"\n",
        "\n",
        "# Model (Gemini 3 Flash, preview)\n",
        "MODEL_ID = \"gemini-3-flash-preview\"\n",
        "\n",
        "# I/O\n",
        "INPUT_DIR = Path(\"/content/Cleaned_texts\")\n",
        "OUTPUT_DIR = Path(\"/content/Gemini_Corrected_texts\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Processing controls\n",
        "TARGET_CHUNK_SIZE = 5000\n",
        "MAX_CONCURRENCY = 5\n",
        "MAX_RETRIES = 5\n",
        "INITIAL_BACKOFF = 2.0  # seconds\n",
        "\n",
        "# Safety settings (loose)\n",
        "SAFETY_SETTINGS = [\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "]\n",
        "\n",
        "# --- Prompts ---\n",
        "SYSTEM_STEER = (\n",
        "    \"You are a specialized OCR correction engine for early modern Latin. \"\n",
        "    \"Output ONLY the corrected text. Do not provide lists of changes, explanations, or commentary.\"\n",
        ")\n",
        "\n",
        "CORRECTION_PROMPT = \"\"\"\n",
        "TASK: Correct ONLY OCR errors in the input text. Do not rewrite or alter authentic early modern usage.\n",
        "\n",
        "RULES:\n",
        "1. Preserve original wording, grammar, and syntax. Fix ONLY misreads, broken words, or OCR punctuation noise.\n",
        "2. Minimal normalization: u=vowel, v=consonant; use 'i' only (no 'j'); ae/oe ligatures; replace &amp; with 'et'.\n",
        "3. DE-HYPHENATE: Join all words broken across line breaks. Remove the hyphen and merge the fragments.\n",
        "4. Keep paragraph breaks and capitalization exactly as in the input.\n",
        "5. Remove page furniture (headers, catchwords, signature marks like 'A ij').\n",
        "6. Preserve exotic/Chinese romanizations (e.g., 'Tai Ki Gin') exactly as printed.\n",
        "7. CRITICAL: Output ONLY the corrected text. No explanations. No bullet points.\n",
        "\n",
        "TEXT TO CORRECT:\n",
        "\"\"\".strip()\n",
        "\n",
        "# -----------------------------\n",
        "# Client init\n",
        "# -----------------------------\n",
        "def init_client():\n",
        "    if USE_VERTEX:\n",
        "        if not os.path.exists(SERVICE_ACCOUNT_JSON):\n",
        "            raise FileNotFoundError(f\"Credentials file not found: {SERVICE_ACCOUNT_JSON}\")\n",
        "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = SERVICE_ACCOUNT_JSON\n",
        "        aclient = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION).aio\n",
        "        print(f\"Gemini model '{MODEL_ID}' via Vertex AI initialized.\")\n",
        "        return aclient\n",
        "\n",
        "    api_key = os.getenv(\"GOOGLE_API_KEY\") or os.getenv(\"GEMINI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"Set GOOGLE_API_KEY or GEMINI_API_KEY for Developer API mode.\")\n",
        "    aclient = genai.Client(api_key=api_key).aio\n",
        "    print(f\"Gemini model '{MODEL_ID}' via Developer API initialized.\")\n",
        "    return aclient\n",
        "\n",
        "# -----------------------------\n",
        "# Utils\n",
        "# -----------------------------\n",
        "def smart_chunk_text(text: str, target_size: int) -> List[str]:\n",
        "    paras = text.split(\"\\n\\n\")\n",
        "    chunks, current, clen = [], [], 0\n",
        "    for p in paras:\n",
        "        if clen + len(p) > target_size and current:\n",
        "            chunks.append(\"\\n\\n\".join(current))\n",
        "            current, clen = [], 0\n",
        "        current.append(p)\n",
        "        clen += len(p)\n",
        "    if current:\n",
        "        chunks.append(\"\\n\\n\".join(current))\n",
        "    return chunks\n",
        "\n",
        "async def call_with_retries(coro_factory):\n",
        "    attempt = 0\n",
        "    backoff = INITIAL_BACKOFF\n",
        "    last_exc = None\n",
        "    while attempt <= MAX_RETRIES:\n",
        "        try:\n",
        "            return await coro_factory()\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "            msg = str(e)\n",
        "            if \"429\" in msg or \"500\" in msg or \"503\" in msg:\n",
        "                attempt += 1\n",
        "                if attempt > MAX_RETRIES:\n",
        "                    break\n",
        "                await asyncio.sleep(backoff)\n",
        "                backoff *= 2\n",
        "            else:\n",
        "                raise\n",
        "    raise last_exc\n",
        "\n",
        "# -----------------------------\n",
        "# Core correction\n",
        "# -----------------------------\n",
        "async def correct_ocr_chunk_async(\n",
        "    aclient: genai.Client,\n",
        "    chunk_text: str,\n",
        "    chunk_id: int,\n",
        "    filename: str,\n",
        "    semaphore: asyncio.Semaphore,\n",
        ") -> Tuple[int, str]:\n",
        "    \"\"\"\n",
        "    Backward-compatible generate_content call: no `system_instruction` kwarg.\n",
        "    We inline SYSTEM_STEER + CORRECTION_PROMPT before the chunk.\n",
        "    \"\"\"\n",
        "    # Inline steer + rules + chunk into the single user message\n",
        "    full_text = f\"{SYSTEM_STEER}\\n\\n{CORRECTION_PROMPT}\\n{chunk_text}\"\n",
        "\n",
        "    async def single_call():\n",
        "        async with semaphore:\n",
        "            return await aclient.models.generate_content(\n",
        "                model=MODEL_ID,\n",
        "                contents=full_text,\n",
        "                safety_settings=SAFETY_SETTINGS,\n",
        "                # Use a plain dict for config to maximize compatibility across SDK versions\n",
        "                config={\n",
        "                    \"temperature\": 0.1,\n",
        "                    \"max_output_tokens\": 8192,\n",
        "                    \"response_mime_type\": \"text/plain\",\n",
        "                },\n",
        "            )\n",
        "\n",
        "    try:\n",
        "        response = await call_with_retries(single_call)\n",
        "\n",
        "        # Prefer the convenience accessor; fall back to stitching parts if needed\n",
        "        corrected = (getattr(response, \"text\", \"\") or \"\").strip()\n",
        "        if not corrected and getattr(response, \"candidates\", None):\n",
        "            try:\n",
        "                parts = response.candidates[0].content.parts\n",
        "                corrected = \"\".join(getattr(p, \"text\", \"\") for p in parts).strip()\n",
        "            except Exception:\n",
        "                corrected = \"\"\n",
        "\n",
        "        if not corrected:\n",
        "            corrected = chunk_text\n",
        "\n",
        "        print(f\"  [Chunk {chunk_id:03d}] ✓ {filename}\")\n",
        "        return chunk_id, corrected\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  [ERROR] {filename} Chunk {chunk_id}: {e} -> returning original\")\n",
        "        return chunk_id, chunk_text\n",
        "\n",
        "async def process_file(file_path: Path, aclient, semaphore: asyncio.Semaphore):\n",
        "    print(f\"\\nSTARTING FILE: {file_path.name}\")\n",
        "    content = file_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    chunks = smart_chunk_text(content, TARGET_CHUNK_SIZE)\n",
        "    tasks = [\n",
        "        correct_ocr_chunk_async(aclient, c, i, file_path.name, semaphore)\n",
        "        for i, c in enumerate(chunks)\n",
        "    ]\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    results.sort(key=lambda x: x[0])\n",
        "    final_text = \"\\n\\n\".join(text for _, text in results)\n",
        "\n",
        "    output_path = OUTPUT_DIR / f\"Gemini_{file_path.name}\"\n",
        "    output_path.write_text(final_text, encoding=\"utf-8\")\n",
        "    print(f\"DONE: {file_path.name}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n",
        "async def main():\n",
        "    aclient = init_client()\n",
        "    try:\n",
        "        if not INPUT_DIR.exists():\n",
        "            print(f\"Input directory {INPUT_DIR} not found.\")\n",
        "            return\n",
        "\n",
        "        files = sorted(INPUT_DIR.glob(\"*.txt\"))\n",
        "        if not files:\n",
        "            print(\"No .txt files found.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Found {len(files)} files. Processing chunks in parallel...\")\n",
        "        semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
        "\n",
        "        for file_path in files:\n",
        "            await process_file(file_path, aclient, semaphore)\n",
        "\n",
        "        print(f\"\\n--- ALL TASKS COMPLETE. Check folder: {OUTPUT_DIR} ---\")\n",
        "    finally:\n",
        "        await aclient.aclose()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIX8MnCg8JKA",
        "outputId": "e30b452e-3b44-46a4-ea6c-4ed72100e984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini model 'gemini-3-flash-preview' via Vertex AI initialized.\n",
            "Found 1 files. Processing chunks in parallel...\n",
            "\n",
            "STARTING FILE: bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt\n",
            "  [ERROR] bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt Chunk 0: AsyncModels.generate_content() got an unexpected keyword argument 'safety_settings' -> returning original\n",
            "  [ERROR] bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt Chunk 1: AsyncModels.generate_content() got an unexpected keyword argument 'safety_settings' -> returning original\n",
            "  [ERROR] bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt Chunk 2: AsyncModels.generate_content() got an unexpected keyword argument 'safety_settings' -> returning original\n",
            "  [ERROR] bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt Chunk 3: AsyncModels.generate_content() got an unexpected keyword argument 'safety_settings' -> returning original\n",
            "  [ERROR] bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt Chunk 4: AsyncModels.generate_content() got an unexpected keyword argument 'safety_settings' -> returning original\n",
            "  [ERROR] bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt Chunk 5: AsyncModels.generate_content() got an unexpected keyword argument 'safety_settings' -> returning original\n",
            "  [ERROR] bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt Chunk 6: AsyncModels.generate_content() got an unexpected keyword argument 'safety_settings' -> returning original\n",
            "  [ERROR] bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt Chunk 7: AsyncModels.generate_content() got an unexpected keyword argument 'safety_settings' -> returning original\n",
            "DONE: bub_gb_vrNA0Xt0g3UC_djvu_Cleaned.txt\n",
            "\n",
            "--- ALL TASKS COMPLETE. Check folder: /content/Gemini_Corrected_texts ---\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjQ6E+2DdxC7iLByrKwjtY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}