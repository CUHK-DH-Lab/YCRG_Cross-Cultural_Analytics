{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsZyo6J3dppxS5pwsv5Npq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CUHK-DH-Lab/YCRG_Cross-Cultural_Analytics/blob/main/EEBO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code downloads the China related texts from EEBO-TCP, except restricted titles which are only available through ProQuest (see CSV file\n",
        "\n",
        " Practical implications for your Colab workflows\n",
        "✔ You can download:\n",
        "\n",
        "All Status = Free items\n",
        "These include all of EEBO‑TCP Phase I & Phase II of the public release\n",
        "\n",
        "❌ You cannot download:\n",
        "\n",
        "Any Status = Restricted items\n",
        "Their XML files will not be present in the GitHub repositories\n",
        "Your script will automatically skip them because the HTTP request returns 404"
      ],
      "metadata": {
        "id": "Ol00RJB5qOkX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeUmsCsRnf_C",
        "outputId": "ae54a7de-50a9-4c43-d16d-5b7b37d11fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing A00002 …\n",
            "Processing A00005 …\n",
            "Done. Check /content/xml and /content/txt.\n"
          ]
        }
      ],
      "source": [
        "# Minimal Colab-ready cell: download the first 2 EEBO-TCP texts and save as .txt\n",
        "\n",
        "import os, re, requests\n",
        "import pandas as pd\n",
        "from lxml import etree\n",
        "\n",
        "# --- Setup output folders ---\n",
        "os.makedirs(\"xml\", exist_ok=True)\n",
        "os.makedirs(\"txt\", exist_ok=True)\n",
        "\n",
        "# --- Load master list of TCP IDs (Phase I & II) ---\n",
        "CSV_URL = \"https://raw.githubusercontent.com/textcreationpartnership/Texts/master/TCP.csv\"\n",
        "df = pd.read_csv(CSV_URL)\n",
        "\n",
        "# Take the first 2 IDs (you can change [:2] to [:N] for more)\n",
        "tcp_ids = df[\"TCP\"].dropna().astype(str).tolist()[:2]\n",
        "\n",
        "def tei_to_text(xml_bytes):\n",
        "    \"\"\"Simple TEI (EEBO-TCP) XML → plain text converter.\"\"\"\n",
        "    parser = etree.XMLParser(recover=True)\n",
        "    root = etree.fromstring(xml_bytes, parser)\n",
        "\n",
        "    ns = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
        "    body = root.find(\".//tei:text\", namespaces=ns)\n",
        "    if body is None:\n",
        "        body = root.find(\".//tei:body\", namespaces=ns)\n",
        "    if body is None:\n",
        "        body = root  # fallback\n",
        "\n",
        "    # Join all visible text; normalize whitespace\n",
        "    text = \" \".join(body.itertext())\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "for tid in tcp_ids:\n",
        "    print(f\"Processing {tid} …\")\n",
        "    xml_url = f\"https://raw.githubusercontent.com/textcreationpartnership/{tid}/master/{tid}.xml\"\n",
        "\n",
        "    r = requests.get(xml_url, timeout=60)\n",
        "    if r.status_code != 200:\n",
        "        print(f\"  → XML not found (HTTP {r.status_code}); skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Save XML\n",
        "    xml_path = f\"xml/{tid}.xml\"\n",
        "    with open(xml_path, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "    # Convert to TXT (fallback to rough tag strip if parsing fails)\n",
        "    try:\n",
        "        txt = tei_to_text(r.content)\n",
        "    except Exception:\n",
        "        print(\"  → parse error; using raw fallback.\")\n",
        "        txt = re.sub(rb\"<[^>]+>\", b\"\", r.content).decode(\"utf-8\", \"replace\")\n",
        "\n",
        "    with open(f\"txt/{tid}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(txt)\n",
        "\n",
        "print(\"Done. Check /content/xml and /content/txt.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only China related texts"
      ],
      "metadata": {
        "id": "yQ8f8HOpodmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-ready: Select only EEBO-TCP texts related to China (Phases I & II) and convert XML -> TXT\n",
        "\n",
        "import os, re, requests, pandas as pd\n",
        "from lxml import etree\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --------------------------\n",
        "# 1) Settings\n",
        "# --------------------------\n",
        "CSV_URL = \"https://raw.githubusercontent.com/textcreationpartnership/Texts/master/TCP.csv\"  # master list of EEBO-TCP IDs (I & II)  (TCP repo)  [source]\n",
        "# Each TCP ID's raw XML is at: https://raw.githubusercontent.com/textcreationpartnership/<ID>/master/<ID>.xml  [source]\n",
        "\n",
        "OUT_XML_DIR = \"xml_china\"\n",
        "OUT_TXT_DIR = \"txt_china\"\n",
        "os.makedirs(OUT_XML_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_TXT_DIR, exist_ok=True)\n",
        "\n",
        "# China-related keyword set (lowercase). Feel free to extend/tweak:\n",
        "KEYWORDS = {\n",
        "    # direct and common\n",
        "    \"china\", \"chinese\", \"pekin\", \"peking\", \"beijing\", \"nanjing\", \"nanquin\", \"nanking\", \"canton\", \"guangzhou\",\n",
        "    \"fujian\", \"foochow\", \"hangzhou\", \"hangchow\", \"nanchang\", \"yangtze\", \"yang-tsze\",\n",
        "    # historical/latinized/variant forms seen in early modern texts\n",
        "    \"chyna\", \"chine\", \"sinae\", \"sina\", \"sinensis\", \"sinens.\", \"cathay\", \"cataya\", \"cathaia\", \"quinsay\", \"quinsai\",\n",
        "    # region/adjoining terms often used in China-focused travel or Jesuit materials\n",
        "    \"tartar\", \"tartary\", \"manchu\", \"mantchou\", \"mandarin\", \"konfucius\", \"confucius\"\n",
        "}\n",
        "\n",
        "# --------------------------\n",
        "# 2) Helpers\n",
        "# --------------------------\n",
        "def tei_to_text(xml_bytes: bytes) -> str:\n",
        "    \"\"\"\n",
        "    Light TEI -> text extraction:\n",
        "    - Prefer the <text>/<body> block\n",
        "    - Flatten to readable paragraphs\n",
        "    - Normalize whitespace\n",
        "    \"\"\"\n",
        "    parser = etree.XMLParser(recover=True, resolve_entities=False)\n",
        "    root = etree.fromstring(xml_bytes, parser=parser)\n",
        "    ns = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
        "\n",
        "    body = root.find(\".//tei:text\", namespaces=ns)\n",
        "    if body is None:\n",
        "        body = root.find(\".//tei:body\", namespaces=ns)\n",
        "    if body is None:\n",
        "        body = root\n",
        "\n",
        "    # Drop notes/running headers/page/col breaks to de-clutter\n",
        "    for xp in [\".//tei:note\", \".//tei:fw\", \".//tei:pb\", \".//tei:cb\", \".//tei:lb\", \".//tei:gap\"]:\n",
        "        for el in body.findall(xp, ns):\n",
        "            parent = el.getparent()\n",
        "            if parent is not None:\n",
        "                parent.remove(el)\n",
        "\n",
        "    chunks = []\n",
        "    def walk(node):\n",
        "        if not isinstance(node.tag, str):\n",
        "            return\n",
        "        if node.text:\n",
        "            chunks.append(node.text)\n",
        "        for child in node:\n",
        "            walk(child)\n",
        "            if child.tail:\n",
        "                chunks.append(child.tail)\n",
        "\n",
        "    walk(body)\n",
        "    text = \"\".join(chunks)\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "def row_matches_china(row: pd.Series, keyword_set=KEYWORDS) -> bool:\n",
        "    \"\"\"\n",
        "    Try to match on Title if available; otherwise search across all string cells in the row.\n",
        "    Matching is case-insensitive and uses substring containment for any keyword.\n",
        "    \"\"\"\n",
        "    # Prefer Title column if present:\n",
        "    for col in row.index:\n",
        "        if str(col).strip().lower() in {\"title\", \"main title\", \"document title\"}:\n",
        "            val = str(row[col]).lower()\n",
        "            return any(k in val for k in keyword_set)\n",
        "\n",
        "    # Fallback: concatenate all strings in the row:\n",
        "    vals = \" | \".join([str(v) for v in row.values if isinstance(v, (str, int, float))]).lower()\n",
        "    return any(k in vals for k in keyword_set)\n",
        "\n",
        "# --------------------------\n",
        "# 3) Load master list and filter\n",
        "# --------------------------\n",
        "print(\"Downloading TCP.csv (official master list of EEBO-TCP texts)…\")\n",
        "df = pd.read_csv(CSV_URL)\n",
        "\n",
        "# Basic sanity: ensure TCP ID column exists\n",
        "tcp_col = None\n",
        "for c in df.columns:\n",
        "    if str(c).strip().lower() == \"tcp\":\n",
        "        tcp_col = c\n",
        "        break\n",
        "if tcp_col is None:\n",
        "    raise RuntimeError(\"Could not find a 'TCP' column in TCP.csv. Inspect df.columns to adapt the code.\")\n",
        "\n",
        "# Filter to China-related\n",
        "mask = df.apply(row_matches_china, axis=1)\n",
        "df_china = df[mask].copy()\n",
        "df_china.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(f\"Matched {len(df_china)} China-related EEBO-TCP records.\")\n",
        "# Save the selection metadata for reference\n",
        "df_china.to_csv(\"china_selection.csv\", index=False)\n",
        "\n",
        "# --------------------------\n",
        "# 4) Download only those XMLs and convert to TXT\n",
        "# --------------------------\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"Colab-China-EEBO-TCP/1.0\"})\n",
        "\n",
        "def fetch_xml(tcp_id: str) -> bytes:\n",
        "    url = f\"https://raw.githubusercontent.com/textcreationpartnership/{tcp_id}/master/{tcp_id}.xml\"\n",
        "    r = session.get(url, timeout=60)\n",
        "    if r.status_code == 200:\n",
        "        return r.content\n",
        "    else:\n",
        "        return b\"\"\n",
        "\n",
        "# Iterate and convert\n",
        "downloaded, converted = 0, 0\n",
        "for _, row in tqdm(df_china.iterrows(), total=len(df_china), desc=\"China subset\"):\n",
        "    tcp_id = str(row[tcp_col]).strip()\n",
        "    if not tcp_id:\n",
        "        continue\n",
        "\n",
        "    xml_path = os.path.join(OUT_XML_DIR, f\"{tcp_id}.xml\")\n",
        "    txt_path = os.path.join(OUT_TXT_DIR, f\"{tcp_id}.txt\")\n",
        "\n",
        "    # Skip if already done\n",
        "    if os.path.exists(txt_path):\n",
        "        continue\n",
        "\n",
        "    # Download XML\n",
        "    if not os.path.exists(xml_path):\n",
        "        xml_bytes = fetch_xml(tcp_id)\n",
        "        if not xml_bytes:\n",
        "            # Not all IDs necessarily have public GitHub XML (rare), skip gracefully\n",
        "            continue\n",
        "        with open(xml_path, \"wb\") as f:\n",
        "            f.write(xml_bytes)\n",
        "        downloaded += 1\n",
        "    else:\n",
        "        with open(xml_path, \"rb\") as f:\n",
        "            xml_bytes = f.read()\n",
        "\n",
        "    # Convert TEI -> TXT\n",
        "    try:\n",
        "        text = tei_to_text(xml_bytes)\n",
        "    except Exception:\n",
        "        # Fallback: crude tag strip\n",
        "        text = re.sub(rb\"<[^>]+>\", b\"\", xml_bytes).decode(\"utf-8\", \"replace\")\n",
        "\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "    converted += 1\n",
        "\n",
        "print(f\"\\nDone. Downloaded: {downloaded} | Converted to TXT: {converted}\")\n",
        "print(f\"XML saved in: {OUT_XML_DIR}\")\n",
        "print(f\"TXT saved in: {OUT_TXT_DIR}\")\n",
        "print(\"Matched metadata saved to: china_selection.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejd-Ms4cofxD",
        "outputId": "2fd96d07-03b4-4d18-e277-7275cbd428d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading TCP.csv (official master list of EEBO-TCP texts)…\n",
            "Matched 208 China-related EEBO-TCP records.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "China subset: 100%|██████████| 208/208 [00:50<00:00,  4.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done. Downloaded: 208 | Converted to TXT: 208\n",
            "XML saved in: xml_china\n",
            "TXT saved in: txt_china\n",
            "Matched metadata saved to: china_selection.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab cell: Download ALL China-related EEBO-TCP XMLs from china_selection.csv, convert to TXT, and save.\n",
        "\n",
        "import os, re, requests, pandas as pd\n",
        "from lxml import etree\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========= Settings =========\n",
        "SELECTION_CSV = \"china_selection.csv\"   # produced by the earlier filtering step\n",
        "XML_DIR = \"china_xml\"\n",
        "TXT_DIR = \"china_txt\"\n",
        "TIMEOUT = 60\n",
        "\n",
        "os.makedirs(XML_DIR, exist_ok=True)\n",
        "os.makedirs(TXT_DIR, exist_ok=True)\n",
        "\n",
        "# ========= Helpers =========\n",
        "def tei_to_text(xml_bytes: bytes) -> str:\n",
        "    \"\"\"\n",
        "    TEI (EEBO-TCP) XML -> plain text\n",
        "    - Prefer <text>/<body>\n",
        "    - Remove TEI non-content clutter (notes, page breaks, etc.)\n",
        "    - Normalize whitespace\n",
        "    \"\"\"\n",
        "    parser = etree.XMLParser(recover=True, resolve_entities=False)\n",
        "    root = etree.fromstring(xml_bytes, parser=parser)\n",
        "    ns = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}\n",
        "\n",
        "    body = root.find(\".//tei:text\", namespaces=ns)\n",
        "    if body is None:\n",
        "        body = root.find(\".//tei:body\", namespaces=ns)\n",
        "    if body is None:\n",
        "        body = root\n",
        "\n",
        "    # remove clutter\n",
        "    for xp in [\".//tei:note\", \".//tei:fw\", \".//tei:pb\", \".//tei:cb\", \".//tei:lb\", \".//tei:gap\"]:\n",
        "        for el in body.findall(xp, ns):\n",
        "            parent = el.getparent()\n",
        "            if parent is not None:\n",
        "                parent.remove(el)\n",
        "\n",
        "    chunks = []\n",
        "    def walk(node):\n",
        "        if not isinstance(node.tag, str):\n",
        "            return\n",
        "        if node.text:\n",
        "            chunks.append(node.text)\n",
        "        for child in node:\n",
        "            walk(child)\n",
        "            if child.tail:\n",
        "                chunks.append(child.tail)\n",
        "\n",
        "    walk(body)\n",
        "    text = \"\".join(chunks)\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "def fetch_xml_by_id(session: requests.Session, tcp_id: str) -> bytes:\n",
        "    # Each EEBO-TCP ID has a public raw TEI XML at this predictable URL\n",
        "    # e.g., https://raw.githubusercontent.com/textcreationpartnership/A12345/master/A12345.xml\n",
        "    url = f\"https://raw.githubusercontent.com/textcreationpartnership/{tcp_id}/master/{tcp_id}.xml\"\n",
        "    resp = session.get(url, timeout=TIMEOUT)\n",
        "    return resp.content if resp.status_code == 200 else b\"\"\n",
        "\n",
        "# ========= Load the selection and find the TCP ID column =========\n",
        "df = pd.read_csv(SELECTION_CSV)\n",
        "tcp_col = None\n",
        "for c in df.columns:\n",
        "    if str(c).strip().lower() == \"tcp\":\n",
        "        tcp_col = c\n",
        "        break\n",
        "if tcp_col is None:\n",
        "    raise RuntimeError(\"Could not find a 'TCP' column in china_selection.csv. Inspect df.columns and update tcp_col accordingly.\")\n",
        "\n",
        "ids = df[tcp_col].dropna().astype(str).str.strip().unique().tolist()\n",
        "print(f\"{len(ids)} TCP IDs found in selection.\")\n",
        "\n",
        "# ========= Download + convert =========\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"Colab-EEBO-China/1.0\"})\n",
        "\n",
        "downloaded, converted, skipped = 0, 0, 0\n",
        "\n",
        "for tid in tqdm(ids, desc=\"Downloading & Converting\"):\n",
        "    xml_path = os.path.join(XML_DIR, f\"{tid}.xml\")\n",
        "    txt_path = os.path.join(TXT_DIR, f\"{tid}.txt\")\n",
        "\n",
        "    if os.path.exists(txt_path):\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    # get XML (from disk or network)\n",
        "    if os.path.exists(xml_path):\n",
        "        with open(xml_path, \"rb\") as f:\n",
        "            xml_bytes = f.read()\n",
        "    else:\n",
        "        xml_bytes = fetch_xml_by_id(session, tid)\n",
        "        if not xml_bytes:\n",
        "            # could not fetch (rare: missing repo/file)\n",
        "            continue\n",
        "        with open(xml_path, \"wb\") as f:\n",
        "            f.write(xml_bytes)\n",
        "        downloaded += 1\n",
        "\n",
        "    # convert to txt\n",
        "    try:\n",
        "        txt = tei_to_text(xml_bytes)\n",
        "    except Exception:\n",
        "        # fallback: crude tag strip\n",
        "        txt = re.sub(rb\"<[^>]+>\", b\"\", xml_bytes).decode(\"utf-8\", \"replace\")\n",
        "\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(txt)\n",
        "    converted += 1\n",
        "\n",
        "print(f\"\\nDone.\")\n",
        "print(f\"Downloaded XML (this run): {downloaded}\")\n",
        "print(f\"Converted to TXT (this run): {converted}\")\n",
        "print(f\"Already existed (skipped): {skipped}\")\n",
        "print(f\"XML folder: {os.path.abspath(XML_DIR)}\")\n",
        "print(f\"TXT folder: {os.path.abspath(TXT_DIR)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPaR3C6EopS7",
        "outputId": "25380985-8714-48cb-d9ac-29b707224446"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "208 TCP IDs found in selection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading & Converting: 100%|██████████| 208/208 [00:16<00:00, 12.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Done.\n",
            "Downloaded XML (this run): 208\n",
            "Converted to TXT (this run): 208\n",
            "Already existed (skipped): 0\n",
            "XML folder: /content/china_xml\n",
            "TXT folder: /content/china_txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fix long s and other abbrevation problem in EEBO"
      ],
      "metadata": {
        "id": "8ivmHrV6HepU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "TXT_DIR = \"china_txt\"\n",
        "\n",
        "# Mapping based on EEBO-TCP transcription standards\n",
        "EXPANSIONS = {\n",
        "    'ſ': 's',          # Long s\n",
        "    # Precomposed Macrons (Commonly used for 'n')\n",
        "    'ā': 'an', 'ē': 'en', 'ī': 'in', 'ō': 'on', 'ū': 'un',\n",
        "    'Ā': 'An', 'Ē': 'En', 'Ī': 'In', 'Ō': 'On', 'Ū': 'Un',\n",
        "\n",
        "    # Scribal Abbreviations / Brevigraphs\n",
        "    'ꝓ': 'pro',        # p-with-flourish\n",
        "    'ꝑ': 'per',        # p-with-stroke (can also be par/por)\n",
        "    'q̄': 'que',        # q-bar (common for -que suffix)\n",
        "    'ꝙ': 'quod',       # q-with-flourish\n",
        "    'ꝿ': 'con',        # Reverse c (con/com)\n",
        "    'ꝸ': 'us',         # 9-like symbol for -us\n",
        "    'yͤ': 'the',        # Thorn with superscript e (the)\n",
        "    'yᵗ': 'that',      # Thorn with superscript t (that)\n",
        "\n",
        "    # Tildes (Often used for double letters)\n",
        "    'm̃': 'mm', 'ñ': 'nn'\n",
        "}\n",
        "\n",
        "# Regex for combining macrons (e.g., any vowel followed by \\u0304)\n",
        "MACRON_REGEX = re.compile(r'([aeiouAEIOU])\\u0304')\n",
        "\n",
        "modified_count = 0\n",
        "total_replacements = 0\n",
        "\n",
        "print(f\"Normalizing EEBO-TCP abbreviations in {TXT_DIR}...\")\n",
        "\n",
        "for filename in os.listdir(TXT_DIR):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(TXT_DIR, filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        new_content = content\n",
        "        file_replacements = 0\n",
        "\n",
        "        # 1. Expand standard mapping\n",
        "        for char, expansion in EXPANSIONS.items():\n",
        "            count = new_content.count(char)\n",
        "            if count > 0:\n",
        "                new_content = new_content.replace(char, expansion)\n",
        "                file_replacements += count\n",
        "\n",
        "        # 2. Expand combining macrons using regex (e.g., o + ̄ -> on)\n",
        "        # We use \\1n to refer to the vowel found and add 'n'\n",
        "        new_content, count = MACRON_REGEX.subn(r'\\1n', new_content)\n",
        "        file_replacements += count\n",
        "\n",
        "        if file_replacements > 0:\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(new_content)\n",
        "            total_replacements += file_replacements\n",
        "            modified_count += 1\n",
        "\n",
        "print(f\"\\nProcessing complete.\")\n",
        "print(f\"Files updated: {modified_count}\")\n",
        "print(f\"Total expansions performed: {total_replacements}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvLxWAF1HhNc",
        "outputId": "82a91be3-e6b1-4b6d-fdcf-8cc5d0dde43a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizing EEBO-TCP abbreviations in china_txt...\n",
            "\n",
            "Processing complete.\n",
            "Files updated: 53\n",
            "Total expansions performed: 4161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"china_txt\", \"zip\", \"china_txt\")  # creates china_txt.zip\n",
        "files.download(\"china_txt.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "W_1Tiv-fpVyY",
        "outputId": "df8eb3f7-1ef6-40f6-aa4d-985aa2a154ce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_60503aae-37ab-4ffc-9f5e-d511d9ced4da\", \"china_txt.zip\", 21298159)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}